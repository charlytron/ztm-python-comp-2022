{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 3\n",
    "\n",
    "We've just created our model below. It's a function that gives us the desired output. Now, we want to __improve__ on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  # our input data\n",
    "y = iris.target # our labels\n",
    "\n",
    "feature_names = iris.feature_names # column names (pre-defined)\n",
    "target_names = iris.target_names # target names (pre-defined)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  # our input data\n",
    "y = iris.target # our labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # 80% training and 20% test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we improve on this model? All we've done is to split our data between `test` and `train`. But we can decide how much data to give the model to train.\n",
    "If we notice above, we've decided that we wanted 80% to be the test data and 20% to be the train data, giving the model less data to learn from, making our accuracy go down, as we're learning from less data.\n",
    "\n",
    "If we give it less data to test, `test_size=0.1`, we'll get 100% accuracy. But that just means that our test size is very small. We only have 15 inputs to test! Because our test data is so small, we can't be sure that our model is accurate because we only tested it `15` times.\n",
    "\n",
    "The trade-off:\n",
    "\n",
    "- How much data do we want to train the model with\n",
    "- How much data do we want to test it with\n",
    "\n",
    "The more data we have, the more we can train our model.\n",
    "\n",
    "## How to improve?\n",
    "\n",
    "With the parameters in `KNeighborsClassifier()`, we can do more, like 4. Our accuracy will be a bit lower. Because we were only testing for 3 neighbors (types) before, now that we've created four nearest dots, we really only need 3.\n",
    "\n",
    "Another thing--if we can collect more data--is to have more columns or parameters. These are sometimes called features, and the more features that a machine has to look at, the more information it can have.\n",
    "\n",
    "That's not to say the more features, the better. But if there was another feature that we can look at, flower-wise, then there is a better model that we can build to predict what type that it is.\n",
    "\n",
    "The most interesting part is the actual algorithm that we use. In this case, we use the `KNeighborsClassifier()` algorithm, but we can use whatever type of algorithm that we want.\n",
    "\n",
    "If we go to the [Scikit-Learn library](https://scikit-learn.org/stable/supervised_learning.html), there are many options to allow our improvements. We can do a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  # our input data\n",
    "y = iris.target # our labels\n",
    "\n",
    "feature_names = iris.feature_names # column names (pre-defined)\n",
    "target_names = iris.target_names # target names (pre-defined)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(30, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  # our input data\n",
    "y = iris.target # our labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # gives nearly 100% accuracy\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this mean that a test size of 0.1 is the best option? Our test size is way small. We only have 15 inputs that are being tested, but we're learning from 135 entries.\n",
    "\n",
    "Because our test data is so small--although we got 15 out of 15--we can't be sure that our model is that accurate because __we only tested it 15 times__.\n",
    "\n",
    "A tradeoff between how much data do we want to train the model with vs. how much do we want to test it with. Companies really value data. __The more data we have, the more we can train our models__.\n",
    "\n",
    "To improve what we have here, with the parameters in `KNeighborsClassifier(n_neighbors=4)` is to increase the number of neighbors, which will lower our accuracy a little. If we were only testing for 3 classes, we would match that with our nearest neighbors. But if our parameter is `(n_neighbors=4)`, what happens is we're creating four __segments (nearest dots)__ for us to evaluate. But we only need three Iris flower types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can we do to improve the model? If we can collect more data, we can maybe have more columns or parameters. These are sometimes called features, and the more features that a machine has to look at, the more information it can have.\n",
    "\n",
    "That's not to say the more features, the better. But if there was another feature that we can look at re the flowers, there is a better model that we can build to predict that flower's type.\n",
    "\n",
    "The most interesting part is the actual algorithm that we use. In this case, we use `KNeighborsClassifier`, but we can use whatever type of algorithm we want.\n",
    "\n",
    "If we go to the [Scikit-Learn Library](https://scikit-learn.org/stable/model_selection.html#model-selection), there are a lot of things we can do, like a __decision tree__, using a decision tree classifier. We simply need to adjust our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75, 4)\n",
      "(75, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data  # our input data\n",
    "y = iris.target # our labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5) # gives nearly 100% accuracy\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "knn = DecisionTreeClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_predict = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9466666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66b6dfc241c8325d69ccfbaf588ee11e398b2b02082dd5e73e37f3a4fd18fba0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
