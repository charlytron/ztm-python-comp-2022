{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping and APIs\n",
    "\n",
    "Most websites really don't want us scraping their data. At times, they'll tell us what we can and cannot scrape. If we write `robots.txt` at the end of a URL, which tells us what we're allowed and not allowed to scrape. It's a matter of staying within ethical boundaries."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#                      ///////\n",
    "#                     //     //\n",
    "#                    //       //\n",
    "#                   //         //                           ///             ///                      ///\n",
    "#                  //           //                                          ///                      ///\n",
    "#                 //     ///     //               //// ///  ///  /// ////   /// ////     /// ////    /// ////\n",
    "#                //   ///   ///   //            //////////  ///  ////////// ///////////  //////////  ///////////\n",
    "#               //   //       //   //          ///     ///  ///  ///        ///      /// ///     /// ///      ///\n",
    "#              //    //       //    //        ///      ///  ///  ///        ///      /// ///     /// ///      ///\n",
    "#             //      //     //      //        ///     ///  ///  ///        ///     ///  ///     /// ///     ///\n",
    "#            //        //   //        //        //////////  ///  ///        //////////   ///     /// //////////\n",
    "#            //         /////         //\n",
    "#            //         /////         //\n",
    "#             //      ///   ///      //\n",
    "#               //////         //////\n",
    "#\n",
    "#\n",
    "#    We thought you'd never make it!\n",
    "#    We hope you feel right at home in this file...unless you're a disallowed subfolder.\n",
    "#    And since you're here, read up on our culture and team: https://www.airbnb.com/careers/departments/engineering\n",
    "#    There's even a bring your robot to work day.\n",
    "\n",
    "\n",
    "User-agent: Googlebot\n",
    "Allow: /calendar/ical/\n",
    "Allow: /.well-known/amphtml/apikey.pub\n",
    "Disallow: /account\n",
    "Disallow: /alumni\n",
    "Disallow: /associates/click\n",
    "Disallow: /api/v1/trebuchet\n",
    "Disallow: /calendar/\n",
    "Disallow: /disaster/lookup\n",
    "Disallow: /email/unsubscribe\n",
    "Disallow: /fix-it\n",
    "Disallow: /fixit\n",
    "Disallow: /forgot_password\n",
    "Disallow: /groups\n",
    "Disallow: /help/search\n",
    "Disallow: /help/feedback\n",
    "Disallow: /home/dashboard\n",
    "Disallow: /inbox\n",
    "Disallow: /logout\n",
    "Disallow: /manage-listing\n",
    "Disallow: /messaging/ajax_already_messaged/\n",
    "Disallow: /my_listings\n",
    "Disallow: /skeleton$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Disallow` indicates what we cannot scrape, and it's a lot, so we'll build a project around hacker news. Their only stipulations are as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "User-Agent: *\n",
    "Disallow: /x?\n",
    "Disallow: /r?\n",
    "Disallow: /vote?\n",
    "Disallow: /reply?\n",
    "Disallow: /submitted?\n",
    "Disallow: /submitlink?\n",
    "Disallow: /threads?\n",
    "Crawl-delay: 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're going to crawl their website, we should have a pause so that we don't constantly hit their site because it will overload their servers.\n",
    "Usually the data that we scrape should not be used for commercial purposes. We'd be using somebody else's hard work to make bank. The crawl delay would make sense because we don't want to break Hacker News.\n",
    "\n",
    "If we go to the [SWAPI website](https://swapi.dev/api/people), though, we get a lot of data for free. We can create an app around the star wars API.\n",
    "\n",
    "<br />\n",
    "\n",
    "![img](./swapi.png)\n",
    "\n",
    "<br />\n",
    "\n",
    "We also have access to the JSON Placeholder, which is good for simple projects.\n",
    "\n",
    "- photos\n",
    "- posts\n",
    "\n",
    "all in JSON format, which is clean. The easiest way to get data from a website is usually through an API. Some websites allow this easy access. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
